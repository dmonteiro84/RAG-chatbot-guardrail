import ragas
from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall
import json
import os

class Evaluator:
    def __init__(self, reference_responses, generated_responses):
        """
        Initialize the evaluator with reference and generated responses.
        :param reference_responses: The correct or expected reference responses.
        :param generated_responses: The responses generated by the LLM.
        """
        self.reference_responses = reference_responses
        self.generated_responses = generated_responses

        # Initialize RAGAs with the LLM and API key
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("Missing OpenAI API Key. Please set the environment variable OPENAI_API_KEY.")
        
        ragas.init(
            llm="gpt-4",
            api_key=api_key
        )

    def evaluate(self):
        """
        Perform evaluation on the dataset of generated and reference responses.
        :return: A dictionary with evaluation scores for faithfulness, relevance, context precision, and recall.
        """
        # Prepare the dataset for evaluation
        dataset = [
            {"generated": gen_resp, "reference": ref_resp}
            for gen_resp, ref_resp in zip(self.generated_responses, self.reference_responses)
        ]

        # Calculate scores for groundedness (factuality) and relevance
        try:
            scores = {
                "faithfulness": faithfulness.score(dataset),
                "answer_relevancy": answer_relevancy.score(dataset),
                "context_precision": context_precision.score(dataset),
                "context_recall": context_recall.score(dataset)
            }
        except Exception as e:
            print(f"Error during evaluation: {e}")
            return None

        return scores

    def print_evaluation(self):
        """
        Print the evaluation results in a readable format.
        """
        evaluation_scores = self.evaluate()
        if evaluation_scores:
            print(json.dumps(evaluation_scores, indent=4))
        else:
            print("Evaluation could not be completed due to an error.")
